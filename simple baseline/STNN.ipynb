{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyUFqA-WmdJ9",
        "outputId": "3e52f137-e553-487c-d5a4-7984bb3782b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "drive.mount('/content/drive')\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/MSc Proj/stnn')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urkwNBg4enzm"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/MSc Proj/stnn\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JNHY1CDYGqY"
      },
      "source": [
        "import psutil\n",
        "import os\n",
        "import pandas as pd\n",
        "import gc\n",
        "import random\n",
        "import numpy as np\n",
        "from utils import *\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from math import radians, cos, sin, asin, sqrt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG6e4EanxIP9"
      },
      "source": [
        "def geodistance(lng1,lat1,lng2,lat2):\n",
        "    if lng1 == lng2 and lat1 == lat2: return 0.0\n",
        "    lng1, lat1, lng2, lat2 = map(radians, [float(lng1), float(lat1), float(lng2), float(lat2)]) \n",
        "    # 经纬度转换成弧度\n",
        "    dlon=lng2-lng1\n",
        "    dlat=lat2-lat1\n",
        "    a=sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
        "    distance=2*asin(sqrt(a))*6371 # 地球平均半径，6371km\n",
        "    # distance=round(distance,0)\n",
        "    return distance  #返回m\n",
        "\n",
        "def drop_na(df,cols):\n",
        "    for col in cols:\n",
        "        df = df.loc[~df[col].isna()]\n",
        "    return df\n",
        "def distance_mapping(df, cluster_num = 60000):\n",
        "    mm_list = []\n",
        "    cols = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude','dropoff_latitude']\n",
        "    for col in cols:\n",
        "        print(f\"************mapping {col}************\")\n",
        "        mm = MinMaxScaler()\n",
        "        df[f'{col}_bin'] = mm.fit_transform(df[col].values.reshape(-1, 1)).reshape(-1)\n",
        "        df[f'{col}_bin'] = (df[f'{col}_bin']*cluster_num).astype(int)\n",
        "        df[f'{col}_bin'] = (df[f'{col}_bin'].astype(float))/cluster_num\n",
        "        mm_list.append(mm)\n",
        "    return df, mm_list\n",
        "\n",
        "\n",
        "def process_lon_lat(df):\n",
        "    lon, lat = -74.0000, 40.43\n",
        "    range = 0.5\n",
        "    df_processed = df.loc[((lon-range)<=df.pickup_longitude)&((lon+range)>=df.pickup_longitude)&((lat-range/3)<=df.pickup_latitude)&((lat+range)>=df.pickup_latitude)]\n",
        "    return df_processed\n",
        "def drop_trip_time_noise(df):\n",
        "    # 删除旅程是0s的 以及数据集trip_time_in_secs与开始结尾相减不同的数据\n",
        "    df = df.loc[df.trip_time_in_secs!=0]\n",
        "    df[\"trip_period\"] = pd.to_datetime(df[\"dropoff_datetime\"])-pd.to_datetime(df[\"pickup_datetime\"])\n",
        "    df[\"trip_period\"] = df[\"trip_period\"].dt.total_seconds().astype(int)\n",
        "\n",
        "    df = df.loc[df.trip_time_in_secs==df.trip_period]\n",
        "\n",
        "    return df\n",
        "\n",
        "def pregen_dis_time_dataset(data):\n",
        "    data = drop_trip_time_noise(data)\n",
        "    data = process_lon_lat(data)\n",
        "    data = drop_na(data,['dropoff_longitude'])\n",
        "    return data\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7hpLQM5xZhJ"
      },
      "source": [
        "\n",
        "def gen_distance_dataset(path,normalize = None):\n",
        "    base_path = path[:-4]\n",
        "    data = pd.read_csv(path)\n",
        "    data = pregen_dis_time_dataset(data)\n",
        "    # deal with distance\n",
        "    names = [\"pickup_longitude\",\"pickup_latitude\",\"dropoff_longitude\",\"dropoff_latitude\",\"geodistance\",\"trip_distance\"]\n",
        "    distance = data[names]\n",
        "    \n",
        "    if normalize:\n",
        "        final_path = base_path+\"_normalized\"+\"_distance_trip.csv\"\n",
        "        # distance = drop_na(distance,['dropoff_longitude'])\n",
        "        distance,mm_list = distance_mapping(distance)\n",
        "        # names = [\"pickup_longitude\",\"pickup_latitude\",\"dropoff_longitude\",\"dropoff_latitude\",\"geodistance\"]\n",
        "        names = ['pickup_longitude_bin','pickup_latitude_bin', 'dropoff_longitude_bin', 'dropoff_latitude_bin','geodistance']\n",
        "        for name in names:\n",
        "            mean = np.nanmean(distance[name].values)\n",
        "            std = np.nanstd(distance[name].values)\n",
        "            distance[name] = (distance[name]-mean)/std\n",
        "                 \n",
        "    else:\n",
        "        final_path = base_path+\"_distance_trip.csv\"\n",
        "        distance = drop_na(distance,['dropoff_longitude'])\n",
        "        # distance,mm_list = distance_mapping(distance)\n",
        "    print(\"distance_dataset： \",final_path)\n",
        "    distance.to_csv(final_path,index = False)\n",
        "    \n",
        "def gen_time_dataset(path,normalize_target = None):\n",
        "    base_path = path[:-4]\n",
        "    data = pd.read_csv(path)\n",
        "    data = pregen_dis_time_dataset(data)\n",
        "    print(\"********************\",len(data))\n",
        "\n",
        "    # deal with Time\n",
        "    names = [\"pickup_datetime\",\"trip_time_in_secs\"]\n",
        "    times = data[names]\n",
        "\n",
        "    min_date = pd.to_datetime(times[\"pickup_datetime\"]).min()\n",
        "    \n",
        "    times[\"pickup_datetime\"] = pd.to_datetime(times[\"pickup_datetime\"])-min_date\n",
        "    times[\"pickup_datetime\"] = times[\"pickup_datetime\"].dt.total_seconds()\n",
        "    \n",
        "    x_max = times[\"pickup_datetime\"].max()\n",
        "    x_min = times[\"pickup_datetime\"].min()\n",
        "    times[\"pickup_datetime\"] = (times[\"pickup_datetime\"]-x_min)/(x_max-x_min)\n",
        "    \n",
        "    if normalize_target:\n",
        "        final_path = base_path+\"_normalized\"+\"_time_trip.csv\"\n",
        "        names = [\"trip_time_in_secs\"]\n",
        "        for name in names:\n",
        "            mean = times[name].mean()\n",
        "            std = times[name].std()\n",
        "            times[name] = (times[name]-mean)/(std)\n",
        "            print(x_max,x_min)\n",
        "            print(times)\n",
        "    else:\n",
        "        final_path = base_path+\"_time_trip.csv\"\n",
        "    print(f\"*****gen_time_dataset to {final_path}*****\")\n",
        "    times.to_csv(final_path,index = False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlCjh4dYexeI"
      },
      "source": [
        "path = './data/trip_data_1.csv'\n",
        "gen_distance_dataset(path,normalize = True)\n",
        "gen_time_dataset(path,normalize_target = True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ihh4HBnFPgI"
      },
      "source": [
        "class experiment:\n",
        "    def __init__(self,distance_path,time_path):\n",
        "        self.lr = 0.01\n",
        "        self.batch_size = 64\n",
        "        self.momenta = 0.9\n",
        "        self.weight_decay= 1e-5\n",
        "        self.distance_path = distance_path\n",
        "        self.time_path = time_path\n",
        "\n",
        "        self.epochs = 10\n",
        "\n",
        "random_seed()\n",
        "exp = experiment(distance_path=\"./data/trip_data_1_normalized_distance_trip.csv\",time_path=\"./data/trip_data_1_normalized_time_trip.csv\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBsrtAUNjUzy"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class DistModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DistModule,self).__init__()\n",
        "        self.layer1 = torch.nn.Linear(5,32)\n",
        "        self.layer2 = torch.nn.Linear(32,64)\n",
        "        self.layer3 = torch.nn.Linear(64,16)\n",
        "        self.layer4 = torch.nn.Linear(16,1)\n",
        "        # self.Sigmoid = nn.Sigmoid()\n",
        "        # self.Relu = nn.ReLU6()\n",
        "\n",
        "        #初始化\n",
        "        self._initialize_weights()\n",
        "    \n",
        "    def forward(self,x):\n",
        "        x = F.sigmoid(self.layer1(x))\n",
        "        # x = self.Relu(x)\n",
        "        x = F.sigmoid(self.layer2(x))\n",
        "        # x = self.Relu(x)\n",
        "        x = F.sigmoid(self.layer3(x))\n",
        "        # x = self.Relu(x)\n",
        "        output = self.layer4(x)\n",
        "        return output,x\n",
        "\n",
        "    \n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                # nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "class TimeModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TimeModule,self).__init__()\n",
        "        self.layer1 = torch.nn.Linear(16+1,64)\n",
        "        self.layer2 = torch.nn.Linear(64,128)\n",
        "        self.layer3 = torch.nn.Linear(128,20)\n",
        "        self.layer4 = torch.nn.Linear(20,1)\n",
        "        self.Relu = torch.nn.ReLU6()\n",
        "\n",
        "        #初始化\n",
        "        self._initialize_weights()\n",
        "    \n",
        "    def forward(self,x):\n",
        "        x = F.sigmoid(self.layer1(x))\n",
        "        x = F.sigmoid(self.layer2(x))\n",
        "        x = F.sigmoid(self.layer3(x))\n",
        "        # x = self.layer1(x)\n",
        "        # x = self.Relu(x)\n",
        "        # x = self.layer2(x)\n",
        "        # x = self.Relu(x)\n",
        "        # x = self.layer3(x)\n",
        "        # x = self.Relu(x)\n",
        "        return self.layer4(x)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                # nn.init.normal_(m.weight, 0, 0.01)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN4KgiL9e5_u"
      },
      "source": [
        "from torch.optim import SGD, lr_scheduler, Adam\n",
        "def init_distance(exp):\n",
        "    # build data\n",
        "    distance_x = ['pickup_longitude_bin','pickup_latitude_bin', 'dropoff_longitude_bin', 'dropoff_latitude_bin','geodistance']\n",
        "    distance_y = 'trip_distance'\n",
        "\n",
        "    distance = pd.read_csv(exp.distance_path)\n",
        "    x = distance[distance_x].values\n",
        "    y = distance[distance_y].values.reshape(-1,1)\n",
        "\n",
        "    print(x[0])\n",
        "    print(y[0])\n",
        "\n",
        "    distance_ds = TensorDataset(torch.from_numpy(x),torch.from_numpy(y))\n",
        "    print(type(distance_ds))\n",
        "    # 分割成训练集和预测集\n",
        "    n_train = int(len(distance)*0.9)\n",
        "    n_valid = len(distance) - n_train\n",
        "    \n",
        "    distance_ds_train,distance_ds_valid = random_split(distance_ds,[n_train,n_valid])\n",
        "    distance_loader_train,distance_loader_valid = DataLoader(distance_ds_train,batch_size= exp.batch_size),DataLoader(distance_ds_valid,batch_size=exp.batch_size)\n",
        "    \n",
        "    # model\n",
        "    distance_model = DistModule()\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = torch.optim.SGD(distance_model.parameters(),lr = exp.lr,momentum=exp.momenta, weight_decay=exp.weight_decay)\n",
        "\n",
        "    # loss function\n",
        "    criterion = torch.nn.MSELoss(reduce = True,size_average = True)\n",
        "\n",
        "    return distance_model,optimizer,criterion,distance_loader_train,distance_loader_valid\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afVhDEMKjReC"
      },
      "source": [
        "class EarlyStopping(object):\n",
        "    def __init__(self, mode='min', min_delta=0, patience=10, percentage=False):\n",
        "        self.mode = mode\n",
        "        self.min_delta = min_delta\n",
        "        self.patience = patience\n",
        "        self.best = None\n",
        "        self.num_bad_epochs = 0\n",
        "        self.is_better = None\n",
        "        self._init_is_better(mode, min_delta, percentage)\n",
        "\n",
        "        if patience == 0:\n",
        "            self.is_better = lambda a, b: True\n",
        "            self.step = lambda a: False\n",
        "\n",
        "    def step(self, metrics):\n",
        "        if self.best is None:\n",
        "            self.best = metrics\n",
        "            return False\n",
        "\n",
        "        if np.isnan(metrics):\n",
        "            return True\n",
        "\n",
        "        if self.is_better(metrics, self.best):\n",
        "            self.num_bad_epochs = 0\n",
        "            self.best = metrics\n",
        "        else:\n",
        "            self.num_bad_epochs += 1\n",
        "\n",
        "        if self.num_bad_epochs >= self.patience:\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _init_is_better(self, mode, min_delta, percentage):\n",
        "        if mode not in {'min', 'max'}:\n",
        "            raise ValueError('mode ' + mode + ' is unknown!')\n",
        "        if not percentage:\n",
        "            if mode == 'min':\n",
        "                self.is_better = lambda a, best: a < best - min_delta\n",
        "            if mode == 'max':\n",
        "                self.is_better = lambda a, best: a > best + min_delta\n",
        "        else:\n",
        "            if mode == 'min':\n",
        "                self.is_better = lambda a, best: a < best - (\n",
        "                            best * min_delta / 100)\n",
        "            if mode == 'max':\n",
        "                self.is_better = lambda a, best: a > best + (\n",
        "                            best * min_delta / 100)\n",
        "\n",
        "\n",
        "def distance_train(exp,distance_model,optimizer,criterion,distance_loader_train,distance_loader_valid,earlyStopping = None, device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")):\n",
        "    print(\"正在使用:\",device)\n",
        "    timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "    columns = [\"epoch\",\"samples\",\"train_loss\",\"valid_loss\"]\n",
        "    log = pd.DataFrame(columns=columns)\n",
        "\n",
        "    distance_model.to(device)\n",
        "    distance_model.train()\n",
        "\n",
        "    reduceLR = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=5, verbose=True)\n",
        "\n",
        "    print(\"len(distance_loader_train) is \",len(distance_loader_train))\n",
        "\n",
        "    exit_flag = False\n",
        "    for epoch in range(exp.epochs):\n",
        "        if exit_flag:\n",
        "            break\n",
        "        running_loss = 0.0\n",
        "        for i,(inputs,labels) in enumerate(distance_loader_train):\n",
        "            inputs,labels = inputs.to(device),labels.to(device)\n",
        "            inputs = inputs.float()\n",
        "            outputs = distance_model(inputs)[0]\n",
        "            \n",
        "            loss = criterion(outputs,labels.float())\n",
        "            #print(loss.item())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            batchs_numm2print = 3000\n",
        "            if (i+1)% batchs_numm2print ==0:\n",
        "                now_loss = running_loss / batchs_numm2print\n",
        "                valid_loss = distance_evaluate(distance_model,criterion,distance_loader_valid,device=device)\n",
        "                # print(\"In epoch {} and {} samples, train loss is {}, valid loss is {} \".format((epoch+1),(i+1),now_loss,valid_loss))\n",
        "                log = pd.DataFrame({\"epoch\":[epoch+1],\"samples\":[i+1],\"train_loss\":[now_loss],\"valid_loss\":[valid_loss]},columns = [\"epoch\",\"samples\",\"train_loss\",\"valid_loss\"])\n",
        "                #lr衰减设置\n",
        "                reduceLR.step(now_loss)\n",
        "                path = \"./log/distance_\"+timestamp+\".csv\"\n",
        "\n",
        "\n",
        "                if i+1==batchs_numm2print:\n",
        "                    log.to_csv(path,mode=\"a\",index = False,header = True)\n",
        "                else:\n",
        "                    log.to_csv(path,mode=\"a\",index = False,header = False)\n",
        "\n",
        "                    #早停设置\n",
        "                    if (i+1)%9000 ==0:\n",
        "                        print(\"In epoch {} and {} samples, train loss is {}, valid loss is {} \".format((epoch+1),(i+1),now_loss,valid_loss))\n",
        "                        if earlyStopping is not None:\n",
        "                            if earlyStopping.step(valid_loss):\n",
        "                                path = \"./models/distance_\"+datetime.now().strftime('%Y%m%d%H%M%S_earlystop')+\".pth\"\n",
        "                                torch.save(distance_model.state_dict(),path)\n",
        "                                print(\"earlyStopping.... model file already in \",path)\n",
        "                                exit_flag = True\n",
        "                                break\n",
        "\n",
        "                distance_model.train()\n",
        "                running_loss = 0.0\n",
        "    path = \"./models/distance_\"+datetime.now().strftime('%Y%m%d%H%M%S')+\".pth\"\n",
        "    torch.save(distance_model.state_dict(),path)\n",
        "    print(\"最终model file already in \",path)\n",
        "    print(\"训练完成\")\n",
        "    \n",
        "\n",
        "\n",
        "def distance_evaluate(distance_model,criterion,distance_loader_valid,device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")):\n",
        "    distance_model.eval()\n",
        "    total,total_loss = len(distance_loader_valid),0\n",
        "    for i,(inputs,labels) in enumerate(distance_loader_valid):\n",
        "        inputs,labels = inputs.to(device),labels.to(device)\n",
        "        inputs = inputs.float()\n",
        "\n",
        "        outputs = distance_model(inputs)[0]\n",
        "        loss = criterion(outputs,labels.float())\n",
        "        if not np.isnan(loss.item()):\n",
        "            total_loss += loss.item()/total\n",
        "    distance_model.train()\n",
        "    return total_loss"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-hi4yXVaPQr",
        "outputId": "245d598b-1c83-4d42-9a99-9c9fb990c5fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# distance\n",
        "distance_model,optimizer,criterion,distance_loader_train,distance_loader_valid = init_distance(exp)\n",
        "earlyStopping = EarlyStopping(patience=3,mode='min')\n",
        "# distance_train(exp,distance_model,optimizer,criterion,distance_loader_train,distance_loader_valid,earlyStopping)\n",
        "distance_model.load_state_dict(torch.load('./models/distance_20201104072611.pth',map_location=torch.device('cpu')))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.0859372   0.2592786  -0.03087294  0.00859118 -0.04046745]\n",
            "[1.]\n",
            "<class 'torch.utils.data.dataset.TensorDataset'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzpBo5iFnIXw"
      },
      "source": [
        "from tqdm import tqdm\n",
        "def init_time(exp,distance_model):\n",
        "    distance_x = ['pickup_longitude_bin', 'pickup_latitude_bin', 'dropoff_longitude_bin', 'dropoff_latitude_bin',\n",
        "                  'geodistance']\n",
        "    distance_y = 'trip_distance'\n",
        "    distance = pd.read_csv(exp.distance_path)\n",
        "    time = pd.read_csv(exp.time_path)\n",
        "    distance_inputs = distance[distance_x].values\n",
        "    distance_labels = distance[distance_y].values.reshape(-1,1)\n",
        "\n",
        "    distance_ds = TensorDataset(torch.from_numpy(distance_inputs),torch.from_numpy(distance_labels))\n",
        "    distance_dataloader = DataLoader(distance_ds,batch_size=256)\n",
        "    distance_outputs = np.array([[0]*16])\n",
        "\n",
        "    for (inputs,labels) in tqdm(distance_dataloader):\n",
        "        inputs = inputs.float()\n",
        "        outputs = distance_model(inputs)[1]\n",
        "        if torch.cuda.is_available():\n",
        "            distance_outputs = np.concatenate((distance_outputs,outputs.cpu().detach().numpy()),axis = 0)\n",
        "        else:\n",
        "            distance_outputs = np.concatenate((distance_outputs,outputs.detach().numpy()),axis = 0)\n",
        "\n",
        "    np.save(\"distance_outputs_2020_10_04.npy\",distance_outputs)\n",
        "    # distance_outputs = np.load(\"distance_outputs_2020_10_04.npy\")\n",
        "\n",
        "    # concatence\n",
        "    x = np.concatenate((distance_outputs[1:],time[\"pickup_datetime\"].values.reshape(-1,1)),axis = 1)\n",
        "    y = time[\"trip_time_in_secs\"].values\n",
        "    # y = (y-min(y))/(max(y)-min(y))\n",
        "    y = y.reshape(-1,1)\n",
        "    time_ds = TensorDataset(torch.from_numpy(x),torch.from_numpy(y))\n",
        "\n",
        "\n",
        "    # 分割成训练集和预测集\n",
        "    n_train = int(len(x)*0.8)\n",
        "    n_valid = len(x) - n_train\n",
        "    time_ds_train,time_ds_valid = random_split(time_ds,[n_train,n_valid])\n",
        "    time_loader_train,time_loader_valid = DataLoader(time_ds_train,batch_size= exp.batch_size),DataLoader(time_ds_valid,batch_size=exp.batch_size)\n",
        "\n",
        "    # model\n",
        "    time_model = TimeModule()\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = torch.optim.SGD(time_model.parameters(),lr = exp.lr,momentum=exp.momenta, weight_decay=exp.weight_decay)\n",
        "\n",
        "    # loss function\n",
        "    criterion = torch.nn.MSELoss(reduce = True,size_average = True)\n",
        "    # criterion = torch.nn.L1Loss(size_average=True,reduce=True,reduction='mean')\n",
        "\n",
        "    return time_model,optimizer,criterion,time_loader_train,time_loader_valid,x,y"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrlnVIJq1v9C"
      },
      "source": [
        "def time_train(exp,time_model,optimizer,criterion,time_loader_train,time_loader_valid,earlyStopping = None,device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),eval_metric = \"MSE\"):\n",
        "    timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "    columns = [\"epoch\",\"samples\",\"train_loss\",\"valid_loss\"]\n",
        "    log = pd.DataFrame(columns=columns)\n",
        "\n",
        "    time_model.to(device)\n",
        "    time_model.train()\n",
        "\n",
        "    reduceLR = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=3, verbose=True)\n",
        "    exit_flag = False\n",
        "    for epoch in range(exp.epochs):\n",
        "        if exit_flag: break\n",
        "\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for i,(inputs,labels) in enumerate(time_loader_train):\n",
        "            inputs,labels = inputs.to(device),labels.to(device)\n",
        "            inputs = inputs.float()\n",
        "            outputs = time_model(inputs)\n",
        "            loss = criterion(outputs,labels.float())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            batchs_numm2print = 1000\n",
        "            if (i+1)% batchs_numm2print ==0:\n",
        "                now_loss = running_loss / batchs_numm2print\n",
        "                # eval_criterion = torch.nn.MSELoss(reduce = True,size_average = True) if eval_metric==\"MSE\" else torch.nn.L1Loss(size_average=True,reduce=True,reduction='mean')\n",
        "                valid_loss = time_evaluate(time_model,criterion,time_loader_valid,device=device)\n",
        "                print(\"In epoch {} and {} samples, train loss is {}, valid loss is {} \".format((epoch+1),(i+1),now_loss,valid_loss))\n",
        "                log = pd.DataFrame({\"epoch\":[epoch+1],\"samples\":[i+1],\"train_loss\":[now_loss],\"valid_loss_\"+eval_metric:[valid_loss]},columns = [\"epoch\",\"samples\",\"train_loss\",\"valid_loss\"])\n",
        "                \n",
        "                #lr衰减设置\n",
        "                reduceLR.step(now_loss)\n",
        "    \n",
        "                path = \"./log/time_\"+timestamp+\".csv\"\n",
        "                if i+1== batchs_numm2print:\n",
        "                    log.to_csv(path,mode=\"a\",index = False,header = True)\n",
        "                else:\n",
        "                    log.to_csv(path,mode=\"a\",index = False,header = False)\n",
        "\n",
        "\n",
        "                #早停设置\n",
        "                if (i+1)%2000 ==0:\n",
        "                    # print(\"In epoch {} and {} samples, train loss is {}, valid loss is {} \".format((epoch+1),(i+1),now_loss,valid_loss))\n",
        "                    if earlyStopping is not None:\n",
        "                        if earlyStopping.step(valid_loss):\n",
        "                            path = \"./models/time_\"+datetime.now().strftime('%Y%m%d%H%M%S_earlystop')+\".pth\"\n",
        "                            torch.save(distance_model.state_dict(),path)\n",
        "                            print(\"earlyStopping.... model file already in \",path)\n",
        "                            exit_flag = True\n",
        "                            break\n",
        "\n",
        "\n",
        "                time_model.train()\n",
        "                running_loss = 0.0\n",
        "                \n",
        "    \n",
        "    # save the model\n",
        "    path = \"./models/time_\"+timestamp+\".pth\"\n",
        "    torch.save(time_model.state_dict(),path)\n",
        "    print(\"最终model file already in \",path)\n",
        "    print(\"训练完成\")\n",
        "\n",
        "def time_evaluate(time_model,criterion,time_loader_valid,device):\n",
        "    time_model.eval()\n",
        "    total,total_loss = len(time_loader_valid),0\n",
        "    for i,(inputs,labels) in enumerate(time_loader_valid):\n",
        "        inputs,labels = inputs.to(device),labels.to(device)\n",
        "        inputs = inputs.float()\n",
        "\n",
        "        outputs = time_model(inputs)\n",
        "        loss = criterion(outputs,labels.float())\n",
        "        if not np.isnan(loss.item()):\n",
        "            total_loss += loss.item()/total\n",
        "            #print(total_loss,loss.item(),total)\n",
        "    \n",
        "    return total_loss"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBs0yGNvPAYN",
        "outputId": "75569ea6-b1d3-4a10-abe1-5c198a8cfa50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "time_model,optimizer,criterion,time_loader_train,time_loader_valid,x,y = init_time(exp,distance_model)\n",
        "earlyStopping = EarlyStopping(patience=3,mode='min')\n",
        "time_train(exp,time_model,optimizer,criterion,time_loader_train,time_loader_valid,earlyStopping=earlyStopping)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "In epoch 1 and 1000 samples, train loss is 1.0163099154531956, valid loss is 0.9495929360258457 \n",
            "In epoch 1 and 2000 samples, train loss is 0.5194803759157658, valid loss is 0.401748542455842 \n",
            "In epoch 1 and 3000 samples, train loss is 0.4055689973682165, valid loss is 0.3985058731725655 \n",
            "In epoch 1 and 4000 samples, train loss is 0.406518347337842, valid loss is 0.41328939164164646 \n",
            "In epoch 1 and 5000 samples, train loss is 0.40720305593311784, valid loss is 0.4015969354783187 \n",
            "In epoch 1 and 6000 samples, train loss is 0.402608458250761, valid loss is 0.3919850788672313 \n",
            "In epoch 1 and 7000 samples, train loss is 0.4002613793313503, valid loss is 0.3949372367540077 \n",
            "In epoch 1 and 8000 samples, train loss is 0.40009845910966396, valid loss is 0.3941904447668007 \n",
            "In epoch 1 and 9000 samples, train loss is 0.37803781259804964, valid loss is 0.3983582095579088 \n",
            "In epoch 1 and 10000 samples, train loss is 0.3890307732522488, valid loss is 0.3971475448746199 \n",
            "In epoch 1 and 11000 samples, train loss is 0.40784554967284203, valid loss is 0.39541678252035894 \n",
            "In epoch 1 and 12000 samples, train loss is 0.3827299906909466, valid loss is 0.39107461210270766 \n",
            "In epoch 1 and 13000 samples, train loss is 0.38891820679605005, valid loss is 0.39146848134591333 \n",
            "Epoch    13: reducing learning rate of group 0 to 5.0000e-03.\n",
            "In epoch 1 and 14000 samples, train loss is 0.40120147057622674, valid loss is 0.39398406616108067 \n",
            "In epoch 1 and 15000 samples, train loss is 0.39827701000869276, valid loss is 0.39071649903853134 \n",
            "In epoch 1 and 16000 samples, train loss is 0.3966970899403095, valid loss is 0.39347087981790635 \n",
            "In epoch 1 and 17000 samples, train loss is 0.3960155123472214, valid loss is 0.3930187315045464 \n",
            "Epoch    17: reducing learning rate of group 0 to 2.5000e-03.\n",
            "In epoch 1 and 18000 samples, train loss is 0.38792792408168314, valid loss is 0.3932964983546424 \n",
            "earlyStopping.... model file already in  ./models/time_20201104112850_earlystop.pth\n",
            "最终model file already in  ./models/time_20201104112215.pth\n",
            "训练完成\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onLHATXh2QYC"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}