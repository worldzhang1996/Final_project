{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from models import *\n",
    "from experiment import experiment\n",
    "from utils import init_distance,distance_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistModule(\n",
      "  (block_layer1): Block(\n",
      "    (block_layer): Sequential(\n",
      "      (0): Linear(in_features=5, out_features=64, bias=True)\n",
      "      (1): ReLU6()\n",
      "      (2): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (3): ReLU6()\n",
      "      (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (5): ReLU6()\n",
      "      (6): Linear(in_features=64, out_features=16, bias=True)\n",
      "      (7): ReLU6()\n",
      "    )\n",
      "    (short_cut_layer): Sequential(\n",
      "      (0): Linear(in_features=5, out_features=64, bias=True)\n",
      "      (1): Linear(in_features=64, out_features=16, bias=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "  )\n",
      "  (block_layer2): Block(\n",
      "    (block_layer): Sequential(\n",
      "      (0): Linear(in_features=5, out_features=64, bias=True)\n",
      "      (1): ReLU6()\n",
      "      (2): Linear(in_features=64, out_features=256, bias=True)\n",
      "      (3): ReLU6()\n",
      "      (4): Linear(in_features=256, out_features=64, bias=True)\n",
      "      (5): ReLU6()\n",
      "      (6): Linear(in_features=64, out_features=16, bias=True)\n",
      "      (7): ReLU6()\n",
      "    )\n",
      "    (short_cut_layer): Sequential(\n",
      "      (0): Linear(in_features=5, out_features=64, bias=True)\n",
      "      (1): Linear(in_features=64, out_features=16, bias=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "  )\n",
      "  (block_layer3): Block(\n",
      "    (block_layer): Sequential(\n",
      "      (0): Linear(in_features=5, out_features=64, bias=True)\n",
      "      (1): ReLU6()\n",
      "      (2): Linear(in_features=64, out_features=528, bias=True)\n",
      "      (3): ReLU6()\n",
      "      (4): Linear(in_features=528, out_features=64, bias=True)\n",
      "      (5): ReLU6()\n",
      "      (6): Linear(in_features=64, out_features=16, bias=True)\n",
      "      (7): ReLU6()\n",
      "    )\n",
      "    (short_cut_layer): Sequential(\n",
      "      (0): Linear(in_features=5, out_features=64, bias=True)\n",
      "      (1): Linear(in_features=64, out_features=16, bias=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "exp = experiment()\n",
    "model = DistModule()\n",
    "model.load_state_dict(torch.load(\"/Users/zhangshijie/Desktop/COMP5933/experiment/models/distance_best_valid_model.pth\",map_location = torch.device(\"cpu\")))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/python_5933/lib/python3.6/site-packages/torch/quantization/observer.py:121: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  reduce_range will be deprecated in a future release of PyTorch.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistModule(\n",
       "  (block_layer1): Block(\n",
       "    (block_layer): Sequential(\n",
       "      (0): Linear(\n",
       "        in_features=5, out_features=64, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (1): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (2): Linear(\n",
       "        in_features=64, out_features=128, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (3): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (4): Linear(\n",
       "        in_features=128, out_features=64, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (5): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (6): Linear(\n",
       "        in_features=64, out_features=16, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (7): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "    )\n",
       "    (short_cut_layer): Sequential(\n",
       "      (0): Linear(\n",
       "        in_features=5, out_features=64, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (1): Linear(\n",
       "        in_features=64, out_features=16, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (2): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block_layer2): Block(\n",
       "    (block_layer): Sequential(\n",
       "      (0): Linear(\n",
       "        in_features=5, out_features=64, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (1): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (2): Linear(\n",
       "        in_features=64, out_features=256, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (3): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (4): Linear(\n",
       "        in_features=256, out_features=64, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (5): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (6): Linear(\n",
       "        in_features=64, out_features=16, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (7): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "    )\n",
       "    (short_cut_layer): Sequential(\n",
       "      (0): Linear(\n",
       "        in_features=5, out_features=64, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (1): Linear(\n",
       "        in_features=64, out_features=16, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (2): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block_layer3): Block(\n",
       "    (block_layer): Sequential(\n",
       "      (0): Linear(\n",
       "        in_features=5, out_features=64, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (1): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (2): Linear(\n",
       "        in_features=64, out_features=528, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (3): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (4): Linear(\n",
       "        in_features=528, out_features=64, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (5): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (6): Linear(\n",
       "        in_features=64, out_features=16, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (7): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "    )\n",
       "    (short_cut_layer): Sequential(\n",
       "      (0): Linear(\n",
       "        in_features=5, out_features=64, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (1): Linear(\n",
       "        in_features=64, out_features=16, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (2): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(\n",
       "    in_features=16, out_features=1, bias=True\n",
       "    (activation_post_process): HistogramObserver()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quantization config\n",
    "model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "model_prepared = torch.quantization.prepare(model, inplace=True)\n",
    "model_prepared.to(exp.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistModule(\n",
       "  (block_layer1): Block(\n",
       "    (block_layer): Sequential(\n",
       "      (0): Linear(\n",
       "        in_features=5, out_features=64, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (1): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (2): Linear(\n",
       "        in_features=64, out_features=128, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (3): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (4): Linear(\n",
       "        in_features=128, out_features=64, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (5): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (6): Linear(\n",
       "        in_features=64, out_features=16, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (7): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "    )\n",
       "    (short_cut_layer): Sequential(\n",
       "      (0): Linear(\n",
       "        in_features=5, out_features=64, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (1): Linear(\n",
       "        in_features=64, out_features=16, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (2): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block_layer2): Block(\n",
       "    (block_layer): Sequential(\n",
       "      (0): Linear(\n",
       "        in_features=5, out_features=64, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (1): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (2): Linear(\n",
       "        in_features=64, out_features=256, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (3): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (4): Linear(\n",
       "        in_features=256, out_features=64, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (5): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (6): Linear(\n",
       "        in_features=64, out_features=16, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (7): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "    )\n",
       "    (short_cut_layer): Sequential(\n",
       "      (0): Linear(\n",
       "        in_features=5, out_features=64, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (1): Linear(\n",
       "        in_features=64, out_features=16, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (2): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block_layer3): Block(\n",
       "    (block_layer): Sequential(\n",
       "      (0): Linear(\n",
       "        in_features=5, out_features=64, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (1): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (2): Linear(\n",
       "        in_features=64, out_features=528, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (3): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (4): Linear(\n",
       "        in_features=528, out_features=64, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (5): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (6): Linear(\n",
       "        in_features=64, out_features=16, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (7): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "    )\n",
       "    (short_cut_layer): Sequential(\n",
       "      (0): Linear(\n",
       "        in_features=5, out_features=64, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (1): Linear(\n",
       "        in_features=64, out_features=16, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (2): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(\n",
       "    in_features=16, out_features=1, bias=True\n",
       "    (activation_post_process): HistogramObserver()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.08302905  0.25578067 -0.02191378  0.00419462 -0.04019317]\n",
      "[1.]\n",
      "torch.utils.data.dataset.TensorDataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/python_5933/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "distance_model, optimizer, criterion, distance_loader_train, distance_loader_valid = init_distance(exp)\n",
    "for i,(inputs,labels) in enumerate(distance_loader_train):\n",
    "    inputs = inputs.to(exp.device).float()\n",
    "    outputs = model_prepared(inputs)\n",
    "    if i >5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistModule(\n",
       "  (block_layer1): Block(\n",
       "    (block_layer): Sequential(\n",
       "      (0): Linear(\n",
       "        in_features=5, out_features=64, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (1): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (2): Linear(\n",
       "        in_features=64, out_features=128, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (3): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (4): Linear(\n",
       "        in_features=128, out_features=64, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (5): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (6): Linear(\n",
       "        in_features=64, out_features=16, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (7): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "    )\n",
       "    (short_cut_layer): Sequential(\n",
       "      (0): Linear(\n",
       "        in_features=5, out_features=64, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (1): Linear(\n",
       "        in_features=64, out_features=16, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (2): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block_layer2): Block(\n",
       "    (block_layer): Sequential(\n",
       "      (0): Linear(\n",
       "        in_features=5, out_features=64, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (1): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (2): Linear(\n",
       "        in_features=64, out_features=256, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (3): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (4): Linear(\n",
       "        in_features=256, out_features=64, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (5): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (6): Linear(\n",
       "        in_features=64, out_features=16, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (7): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "    )\n",
       "    (short_cut_layer): Sequential(\n",
       "      (0): Linear(\n",
       "        in_features=5, out_features=64, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (1): Linear(\n",
       "        in_features=64, out_features=16, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (2): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block_layer3): Block(\n",
       "    (block_layer): Sequential(\n",
       "      (0): Linear(\n",
       "        in_features=5, out_features=64, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (1): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (2): Linear(\n",
       "        in_features=64, out_features=528, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (3): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (4): Linear(\n",
       "        in_features=528, out_features=64, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (5): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (6): Linear(\n",
       "        in_features=64, out_features=16, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (7): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "    )\n",
       "    (short_cut_layer): Sequential(\n",
       "      (0): Linear(\n",
       "        in_features=5, out_features=64, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (1): Linear(\n",
       "        in_features=64, out_features=16, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (2): ReLU6(\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(\n",
       "    in_features=16, out_features=1, bias=True\n",
       "    (activation_post_process): HistogramObserver()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Training Quantization: Convert done\n",
      "\n",
      " Inverted Residual Block: After fusion and quantization, note fused modules: \n",
      "\n",
      " Block(\n",
      "  (block_layer): Sequential(\n",
      "    (0): QuantizedLinear(in_features=5, out_features=64, scale=0.003103962168097496, zero_point=60, qscheme=torch.per_channel_affine)\n",
      "    (1): QuantizedReLU6()\n",
      "    (2): QuantizedLinear(in_features=64, out_features=128, scale=0.00019127705309074372, zero_point=61, qscheme=torch.per_channel_affine)\n",
      "    (3): QuantizedReLU6()\n",
      "    (4): QuantizedLinear(in_features=128, out_features=64, scale=2.4462131477775984e-05, zero_point=63, qscheme=torch.per_channel_affine)\n",
      "    (5): QuantizedReLU6()\n",
      "    (6): QuantizedLinear(in_features=64, out_features=16, scale=0.0005808728164993227, zero_point=127, qscheme=torch.per_channel_affine)\n",
      "    (7): QuantizedReLU6()\n",
      "  )\n",
      "  (short_cut_layer): Sequential(\n",
      "    (0): QuantizedLinear(in_features=5, out_features=64, scale=0.04166349396109581, zero_point=58, qscheme=torch.per_channel_affine)\n",
      "    (1): QuantizedLinear(in_features=64, out_features=16, scale=0.6058860421180725, zero_point=87, qscheme=torch.per_channel_affine)\n",
      "    (2): QuantizedReLU6()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.quantization.convert(model_prepared, inplace=True)\n",
    "print('Post Training Quantization: Convert done')\n",
    "print('\\n Inverted Residual Block: After fusion and quantization, note fused modules: \\n\\n',model.block_layer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 5])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not run 'quantized::linear' with arguments from the 'CPU' backend. 'quantized::linear' is only available for these backends: [QuantizedCPU, BackendSelect, Named, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, Tracer, Autocast, Batched, VmapMode].\n\nQuantizedCPU: registered at /Users/distiller/project/conda/conda-bld/pytorch_1603729183814/work/aten/src/ATen/native/quantized/cpu/qlinear.cpp:399 [kernel]\nBackendSelect: fallthrough registered at /Users/distiller/project/conda/conda-bld/pytorch_1603729183814/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nNamed: registered at /Users/distiller/project/conda/conda-bld/pytorch_1603729183814/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nAutogradOther: fallthrough registered at /Users/distiller/project/conda/conda-bld/pytorch_1603729183814/work/aten/src/ATen/core/VariableFallbackKernel.cpp:35 [backend fallback]\nAutogradCPU: fallthrough registered at /Users/distiller/project/conda/conda-bld/pytorch_1603729183814/work/aten/src/ATen/core/VariableFallbackKernel.cpp:39 [backend fallback]\nAutogradCUDA: fallthrough registered at /Users/distiller/project/conda/conda-bld/pytorch_1603729183814/work/aten/src/ATen/core/VariableFallbackKernel.cpp:43 [backend fallback]\nAutogradXLA: fallthrough registered at /Users/distiller/project/conda/conda-bld/pytorch_1603729183814/work/aten/src/ATen/core/VariableFallbackKernel.cpp:47 [backend fallback]\nTracer: fallthrough registered at /Users/distiller/project/conda/conda-bld/pytorch_1603729183814/work/torch/csrc/jit/frontend/tracer.cpp:967 [backend fallback]\nAutocast: fallthrough registered at /Users/distiller/project/conda/conda-bld/pytorch_1603729183814/work/aten/src/ATen/autocast_mode.cpp:254 [backend fallback]\nBatched: registered at /Users/distiller/project/conda/conda-bld/pytorch_1603729183814/work/aten/src/ATen/BatchingRegistrations.cpp:511 [backend fallback]\nVmapMode: fallthrough registered at /Users/distiller/project/conda/conda-bld/pytorch_1603729183814/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-16c5770779a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_prepared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/python_5933/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/COMP5933/experiment/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mblock_layer1_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_layer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mblock_layer2_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_layer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mblock_layer3_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_layer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/python_5933/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/COMP5933/experiment/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mblock_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mshort_cut_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_cut_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshort_cut_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/python_5933/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/python_5933/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/python_5933/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/python_5933/lib/python3.6/site-packages/torch/nn/quantized/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         return torch.ops.quantized.linear(\n\u001b[0;32m--> 167\u001b[0;31m             x, self._packed_params._packed_params, self.scale, self.zero_point)\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;31m# ===== Serialization methods =====\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not run 'quantized::linear' with arguments from the 'CPU' backend. 'quantized::linear' is only available for these backends: [QuantizedCPU, BackendSelect, Named, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, Tracer, Autocast, Batched, VmapMode].\n\nQuantizedCPU: registered at /Users/distiller/project/conda/conda-bld/pytorch_1603729183814/work/aten/src/ATen/native/quantized/cpu/qlinear.cpp:399 [kernel]\nBackendSelect: fallthrough registered at /Users/distiller/project/conda/conda-bld/pytorch_1603729183814/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nNamed: registered at /Users/distiller/project/conda/conda-bld/pytorch_1603729183814/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nAutogradOther: fallthrough registered at /Users/distiller/project/conda/conda-bld/pytorch_1603729183814/work/aten/src/ATen/core/VariableFallbackKernel.cpp:35 [backend fallback]\nAutogradCPU: fallthrough registered at /Users/distiller/project/conda/conda-bld/pytorch_1603729183814/work/aten/src/ATen/core/VariableFallbackKernel.cpp:39 [backend fallback]\nAutogradCUDA: fallthrough registered at /Users/distiller/project/conda/conda-bld/pytorch_1603729183814/work/aten/src/ATen/core/VariableFallbackKernel.cpp:43 [backend fallback]\nAutogradXLA: fallthrough registered at /Users/distiller/project/conda/conda-bld/pytorch_1603729183814/work/aten/src/ATen/core/VariableFallbackKernel.cpp:47 [backend fallback]\nTracer: fallthrough registered at /Users/distiller/project/conda/conda-bld/pytorch_1603729183814/work/torch/csrc/jit/frontend/tracer.cpp:967 [backend fallback]\nAutocast: fallthrough registered at /Users/distiller/project/conda/conda-bld/pytorch_1603729183814/work/aten/src/ATen/autocast_mode.cpp:254 [backend fallback]\nBatched: registered at /Users/distiller/project/conda/conda-bld/pytorch_1603729183814/work/aten/src/ATen/BatchingRegistrations.cpp:511 [backend fallback]\nVmapMode: fallthrough registered at /Users/distiller/project/conda/conda-bld/pytorch_1603729183814/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "for i,(inputs,labels) in enumerate(distance_loader_train):\n",
    "    inputs = inputs.to(exp.device).float()\n",
    "    print(inputs.size())\n",
    "    outputs = model_prepared(inputs)\n",
    "    print(outputs,type(outputs),outputs.size())\n",
    "    if i >5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(64,5)\n",
    "model_prepared(inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python_5933]",
   "language": "python",
   "name": "conda-env-python_5933-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
